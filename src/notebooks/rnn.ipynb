{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "# RNN - Version 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yG_n40gFzf9s",
    "ExecuteTime": {
     "end_time": "2024-09-28T23:23:45.213352Z",
     "start_time": "2024-09-28T23:23:32.176175Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "\n",
    "\n",
    "# print the version of tensorflow\n",
    "print(tf.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T23:23:52.335347Z",
     "start_time": "2024-09-28T23:23:52.315327Z"
    }
   },
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "print(src_path)\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "from shared.globals import *\n",
    "\n",
    "# print some global var just to check\n",
    "print(PAD)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\the_3\\arc-model\\src\n",
      "19\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "Load the arc-agi_training_challenges_bpe Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pD_55cOxLkAb",
    "ExecuteTime": {
     "end_time": "2024-09-28T23:27:16.258946Z",
     "start_time": "2024-09-28T23:27:16.249942Z"
    }
   },
   "source": [
    "# dataset_path = \"../../../data/arc-agi_training_challenges_bpe.csv\"\n",
    "dataset_path = \"C:/Users/the_3/arc-model/data/bpe2048/arc-agi_training_challenges_bpe.csv\""
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add padding on sequences"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T23:27:19.154676Z",
     "start_time": "2024-09-28T23:27:19.140677Z"
    }
   },
   "source": [
    "def pad_sequence(curr_seq_len, curr_seq):\n",
    "    # Calculate number of padding required for the current sequence\n",
    "    num_pads = SEQ_LEN - curr_seq_len\n",
    "\n",
    "    # Add padding at the beggining, followd by the sequence\n",
    "    padded_sequence = [PAD] * num_pads + curr_seq\n",
    "    return padded_sequence"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T23:27:29.796709Z",
     "start_time": "2024-09-28T23:27:22.222452Z"
    }
   },
   "source": [
    "padded_sequences = []\n",
    "\n",
    "with open(dataset_path, mode='r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # skip header\n",
    "\n",
    "    for row in reader:\n",
    "        # Read row as integers\n",
    "        curr_seq = list(map(int, row[3].split(' ')))\n",
    "        curr_seq_len = int(row[2])\n",
    "\n",
    "        # Add padding at the beginning of the sequence\n",
    "        padded_sequences.append(pad_sequence(curr_seq_len, curr_seq))"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to see if:\n",
    "- Padded was added correctly\n",
    "- Our sequence is a list of type int\n",
    "- Sequences length match SEQ_LEN, which is 1500"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T23:27:34.362434Z",
     "start_time": "2024-09-28T23:27:34.345436Z"
    }
   },
   "source": [
    "i = 12\n",
    "print(\"Sequence \", np.array(padded_sequences[i])) # just used np.array for demostrantion\n",
    "print(\"Type: \", type(padded_sequences[i][0]))\n",
    "print(\"Length: \", len(padded_sequences[i]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence  [ 19  19  19 ...  93 384   0]\n",
      "Type:  <class 'int'>\n",
      "Length:  1500\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Process the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### Flatten sequence\n",
    "Flatten the list of padded sequences into a single sequence:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a86OoYtO01go",
    "ExecuteTime": {
     "end_time": "2024-09-28T23:27:43.177944Z",
     "start_time": "2024-09-28T23:27:37.813949Z"
    }
   },
   "source": [
    "flattened_sequence = [token for seq in padded_sequences for token in seq]"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s4f1q3iqY8f"
   },
   "source": [
    "Now convert the flattened sequence to a TensorFlow dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6GMlCe3qzaL9",
    "ExecuteTime": {
     "end_time": "2024-09-28T23:27:49.759491Z",
     "start_time": "2024-09-28T23:27:43.184947Z"
    }
   },
   "source": [
    "tf_dataset = tf.constant(flattened_sequence)\n",
    "tf_dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(90789000,), dtype=int32, numpy=array([19, 19, 19, ...,  0, 12, 14])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uenivzwqsDhp"
   },
   "source": [
    "Batch the dataset to create sequences of length SEQ_LEN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### Create training examples and targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmX_jbgQqfOi"
   },
   "source": [
    "Use `tf.data.Dataset.from_tensor_slices` to get the slices of the list in the form of objects:\n",
    " - TODO:\n",
    " -- read more about this"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WLv5Q_2TC2pc",
    "ExecuteTime": {
     "end_time": "2024-09-28T23:27:49.790858Z",
     "start_time": "2024-09-28T23:27:49.763521Z"
    }
   },
   "source": [
    "sliced_dataset = tf.data.Dataset.from_tensor_slices(tf_dataset)\n",
    "sliced_dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch the dataset to create sequences of length SEQ_LEN:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wd2m3mqkDjRj",
    "ExecuteTime": {
     "end_time": "2024-09-28T23:27:50.168070Z",
     "start_time": "2024-09-28T23:27:49.795396Z"
    }
   },
   "source": [
    "sequences = sliced_dataset.batch(SEQ_LEN, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(3):\n",
    "    print(seq)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 19  19  19 ...  10  46 312], shape=(1500,), dtype=int32)\n",
      "tf.Tensor([ 19  19  19 ...  46 312   7], shape=(1500,), dtype=int32)\n",
      "tf.Tensor([ 19  19  19 ... 312   7   0], shape=(1500,), dtype=int32)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map to input and target sequences:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UopbsKi88tm5",
    "ExecuteTime": {
     "end_time": "2024-09-28T23:27:56.547323Z",
     "start_time": "2024-09-28T23:27:56.536325Z"
    }
   },
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BpdjRO2CzOfZ",
    "ExecuteTime": {
     "end_time": "2024-09-28T23:27:59.002541Z",
     "start_time": "2024-09-28T23:27:58.737072Z"
    }
   },
   "source": [
    "dataset = sequences.map(split_input_target)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GNbw-iR0ymwj",
    "ExecuteTime": {
     "end_time": "2024-09-28T23:28:00.466219Z",
     "start_time": "2024-09-28T23:28:00.152223Z"
    }
   },
   "source": [
    "for input_example, target_example in dataset.take(3):\n",
    "    print(\"Input :\", input_example.numpy())\n",
    "    print(\"Target:\", target_example.numpy())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : [19 19 19 ... 93 10 46]\n",
      "Target: [ 19  19  19 ...  10  46 312]\n",
      "Input : [ 19  19  19 ...  10  46 312]\n",
      "Target: [ 19  19  19 ...  46 312   7]\n",
      "Input : [ 19  19  19 ...  46 312   7]\n",
      "Target: [ 19  19  19 ... 312   7   0]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### Create training batches\n",
    "\n",
    "shuffle the data and pack it into batches."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p2pGotuNzf-S",
    "ExecuteTime": {
     "end_time": "2024-09-28T23:28:05.085467Z",
     "start_time": "2024-09-28T23:28:04.528472Z"
    }
   },
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 1499), dtype=tf.int32, name=None), TensorSpec(shape=(64, 1499), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zHT8cLh7EAsg",
    "ExecuteTime": {
     "end_time": "2024-09-28T23:28:08.669572Z",
     "start_time": "2024-09-28T23:28:08.649545Z"
    }
   },
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = VOCAB_SIZE\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wj8HQ2w8z4iO",
    "ExecuteTime": {
     "end_time": "2024-09-28T23:28:11.697844Z",
     "start_time": "2024-09-28T23:28:11.676706Z"
    }
   },
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__()  # the __init__ on 2.17 passes \"self\" by default.\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True\n",
    "        )\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "      x = inputs\n",
    "      x = self.embedding(x, training=training)\n",
    "      if states is None: # initialized the state with zeros so dont crash on \"Try the model\" step\n",
    "          batch_size = tf.shape(x)[0]\n",
    "          states = tf.zeros([batch_size, self.gru.units], dtype=tf.float32)\n",
    "      x, states = self.gru(x, initial_state=states, training=training)\n",
    "      x = self.dense(x, training=training)\n",
    "\n",
    "      if return_state:\n",
    "          return x, states\n",
    "      else:\n",
    "          return x"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IX58Xj9z47Aw",
    "ExecuteTime": {
     "end_time": "2024-09-28T23:28:16.719431Z",
     "start_time": "2024-09-28T23:28:14.160988Z"
    }
   },
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## Try the model\n",
    "\n",
    "Now run the model to see that it behaves as expected.\n",
    "\n",
    "First check the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C-_70kKAPrPU",
    "ExecuteTime": {
     "end_time": "2024-09-28T23:28:55.951814Z",
     "start_time": "2024-09-28T23:28:20.457916Z"
    }
   },
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ],
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Exception encountered when calling layer \"gru\" \"                 f\"(type GRU).\n\n{{function_node __wrapped__CudnnRNN_device_/job:localhost/replica:0/task:0/device:GPU:0}} Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 256, 1024, 1, 1499, 64, 0]  [Op:CudnnRNN]\n\nCall arguments received by layer \"gru\" \"                 f\"(type GRU):\n  • inputs=tf.Tensor(shape=(64, 1499, 256), dtype=float32)\n  • mask=None\n  • training=False\n  • initial_state=['tf.Tensor(shape=(64, 1024), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInternalError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m input_example_batch, target_example_batch \u001B[38;5;129;01min\u001B[39;00m dataset\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m----> 2\u001B[0m     example_batch_predictions \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_example_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(example_batch_predictions\u001B[38;5;241m.\u001B[39mshape, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m# (batch_size, sequence_length, vocab_size)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "Cell \u001B[1;32mIn[23], line 18\u001B[0m, in \u001B[0;36mMyModel.call\u001B[1;34m(self, inputs, states, return_state, training)\u001B[0m\n\u001B[0;32m     16\u001B[0m     batch_size \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mshape(x)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     17\u001B[0m     states \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mzeros([batch_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgru\u001B[38;5;241m.\u001B[39munits], dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m---> 18\u001B[0m x, states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgru\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdense(x, training\u001B[38;5;241m=\u001B[39mtraining)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_state:\n",
      "\u001B[1;31mInternalError\u001B[0m: Exception encountered when calling layer \"gru\" \"                 f\"(type GRU).\n\n{{function_node __wrapped__CudnnRNN_device_/job:localhost/replica:0/task:0/device:GPU:0}} Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 256, 1024, 1, 1499, 64, 0]  [Op:CudnnRNN]\n\nCall arguments received by layer \"gru\" \"                 f\"(type GRU):\n  • inputs=tf.Tensor(shape=(64, 1499, 256), dtype=float32)\n  • mask=None\n  • training=False\n  • initial_state=['tf.Tensor(shape=(64, 1024), dtype=float32)']"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vPGmAAXmVLGC"
   },
   "source": [
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZOeWdgxNFDXq"
   },
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4HrXTACTdzY-"
   },
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkvUIneTFiow"
   },
   "source": [
    "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:\n",
    "\n",
    "- TODO:\n",
    "    ask more about this"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MAJfS5YoFiHf"
   },
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeOXriLcymww"
   },
   "source": [
    "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DDl1_Een6rL0"
   },
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### Configure checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6XBUUavgF56"
   },
   "source": [
    "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W6fWTriUZP-n"
   },
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\") # added \".weights.h5\" to fix issue\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### Execute the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T13:18:55.071251Z",
     "start_time": "2024-09-05T13:18:55.048252Z"
    },
    "id": "7yGBE2zxMMHs"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T13:47:54.826392Z",
     "start_time": "2024-09-05T13:44:09.081217Z"
    },
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18 23:44:20.033479: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:7: Filling up shuffle buffer (this may take a while): 9992 of 10000\n",
      "2024-09-18 23:44:20.041248: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m 47/945\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m4:37:37\u001B[0m 19s/step - loss: 2.4115"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## Generate Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "The following makes a single step prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T14:19:51.405364Z",
     "start_time": "2024-09-05T14:19:51.334371Z"
    },
    "id": "iSBU1tHmlUSs"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[VOCAB_SIZE])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, temperature=1.0, id_to_token=None):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.id_to_token = id_to_token  # Mapping from token IDs to tokens\n",
    "        self.token_to_id = {v: k for k, v in id_to_token.items()}  # Mapping from tokens to IDs\n",
    "        self.end_seq_id = self.token_to_id['END_SEQ']  # Token ID for 'END_SEQ'\n",
    "\n",
    "    @tf.function\n",
    "    def generate(self, initial_input, max_iter=100, states=None):\n",
    "        # Initialize the list to store output tokens\n",
    "        output_tokens = []\n",
    "\n",
    "        # Ensure initial_input is a tensor of shape [batch_size, sequence_length]\n",
    "        if isinstance(initial_input, list):\n",
    "            input_ids = tf.constant([initial_input], dtype=tf.int32)\n",
    "        elif isinstance(initial_input, tf.Tensor):\n",
    "            input_ids = initial_input\n",
    "        else:\n",
    "            raise ValueError(\"initial_input should be a list or a tf.Tensor of token IDs\")\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            # Run the model to get predicted logits and updated states\n",
    "            predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n",
    "\n",
    "            # Get the logits for the last time step\n",
    "            predicted_logits = predicted_logits[:, -1, :]  # Shape: [batch_size, vocab_size]\n",
    "\n",
    "            # Adjust for temperature\n",
    "            predicted_logits = predicted_logits / self.temperature\n",
    "\n",
    "            # Sample the next token ID\n",
    "            predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "            predicted_ids = tf.squeeze(predicted_ids, axis=-1)  # Shape: [batch_size]\n",
    "\n",
    "            # Append the predicted token ID to the output\n",
    "            output_tokens.append(predicted_ids[0].numpy())  # Assuming batch_size = 1\n",
    "\n",
    "            # Check if END_SEQ token is generated\n",
    "            if predicted_ids[0] == self.end_seq_id:\n",
    "                break\n",
    "\n",
    "            # Prepare the next input (the predicted token)\n",
    "            input_ids = tf.expand_dims(predicted_ids, axis=0)  # Shape: [batch_size, 1]\n",
    "\n",
    "        return output_tokens\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a trained model called `trained_model`\n",
    "# And a mapping from IDs to tokens `id_to_token`\n",
    "\n",
    "# id_to_token example:\n",
    "id_to_token = {\n",
    "    0: 'START_SEQ',\n",
    "    1: 'I',\n",
    "    2: 'am',\n",
    "    3: 'a',\n",
    "    4: 'model',\n",
    "    # ... (other tokens)\n",
    "    14: 'END_SEQ'\n",
    "}\n",
    "\n",
    "# Create the OneStep model instance\n",
    "one_step_model = OneStep(model=trained_model, temperature=1.0, id_to_token=id_to_token)\n",
    "\n",
    "# Initialize input with the START_SEQ token ID\n",
    "initial_input = [one_step_model.token_to_id['START_SEQ']]\n",
    "\n",
    "# Generate tokens\n",
    "generated_tokens = one_step_model.generate(initial_input=initial_input, max_iter=100)\n",
    "\n",
    "# Convert token IDs back to tokens\n",
    "generated_sequence = [one_step_model.id_to_token[token_id] for token_id in generated_tokens]\n",
    "\n",
    "print(\"Generated sequence:\", generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T14:19:54.456584Z",
     "start_time": "2024-09-05T14:19:54.427091Z"
    },
    "id": "fqMOuDutnOxK"
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9yDoa0G3IgQ"
   },
   "source": [
    "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T14:23:44.000100Z",
     "start_time": "2024-09-05T14:23:40.792611Z"
    },
    "id": "ST7PSyk9t1mT"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AM2Uma_-yVIq"
   },
   "source": [
    "The easiest thing you can do to improve the results is to train it for longer (try `EPOCHS = 30`).\n",
    "\n",
    "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OfbI4aULmuj"
   },
   "source": [
    "If you want the model to generate text *faster* the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T14:21:43.320760Z",
     "start_time": "2024-09-05T14:21:38.781768Z"
    },
    "id": "ZkLu7Y8UCMT7"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlUQzwu6EXam"
   },
   "source": [
    "## Export the generator\n",
    "\n",
    "This single-step model can easily be [saved and restored](https://www.tensorflow.org/guide/saved_model), allowing you to use it anywhere a `tf.saved_model` is accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Grk32H_CzsC",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Z9bb_wX6Uuu",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4QwTjAM6A2O"
   },
   "source": [
    "## Advanced: Customized Training\n",
    "\n",
    "The above training procedure is simple, but does not give you much control.\n",
    "It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
    "\n",
    "So now that you've seen how to run the model manually next you'll implement the training loop. This gives a starting point if, for example, you want to implement _curriculum  learning_ to help stabilize the model's open-loop output.\n",
    "\n",
    "The most important part of a custom training loop is the train step function.\n",
    "\n",
    "Use `tf.GradientTape` to track the gradients. You can learn more about this approach by reading the [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
    "\n",
    "The basic procedure is:\n",
    "\n",
    "1. Execute the model and calculate the loss under a `tf.GradientTape`.\n",
    "2. Calculate the updates and apply them to the model using the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0pZ101hjwW0",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Oc-eJALcK8B"
   },
   "source": [
    "The above implementation of the `train_step` method follows [Keras' `train_step` conventions](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This is optional, but it allows you to change the behavior of the train step and still use keras' `Model.compile` and `Model.fit` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKyWiZ_Lj7w5",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U817KUm7knlm",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o694aoBPnEi9",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8nAtKHVoInR"
   },
   "source": [
    "Or if you need more control, you can write your own complete custom training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4tSNwymzf-q",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    mean.reset_states()\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "            print(template)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print()\n",
    "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
    "    print(\"_\"*80)\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
